{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11033723,"sourceType":"datasetVersion","datasetId":6862038}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport copy\nfrom pdf2image import convert_from_path\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:32.971761Z","iopub.execute_input":"2025-03-14T20:08:32.972048Z","iopub.status.idle":"2025-03-14T20:08:39.900835Z","shell.execute_reply.started":"2025-03-14T20:08:32.971997Z","shell.execute_reply":"2025-03-14T20:08:39.900031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CSV_FILE       = \"/kaggle/input/musicsheets/preprocessed_images/dataset.csv\"        # CSV with 'primary_genre' and 'pdf' columns\nPDF_FOLDER     = \"/kaggle/input/musicsheets/preprocessed_images/preprocessed_images/\"       # folder containing PDF files\nIMG_SIZE       = (512, 512)\nBATCH_SIZE     = 12\nVAL_SPLIT      = 0.2\nNUM_EPOCHS     = 10\nLEARNING_RATE  = 1e-3\nRANDOM_SEED    = 42\ntorch.manual_seed(RANDOM_SEED)\n# Create a generator for DataLoader shuffling\ng = torch.Generator()\ng.manual_seed(42)  # Ensures same shuffle order across runs\n\nEARLY_STOP_PATIENCE = 3  # number of epochs with no improvement to stop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:39.901632Z","iopub.execute_input":"2025-03-14T20:08:39.902099Z","iopub.status.idle":"2025-03-14T20:08:39.911739Z","shell.execute_reply.started":"2025-03-14T20:08:39.902070Z","shell.execute_reply":"2025-03-14T20:08:39.910729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data loading pipeline","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(CSV_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:42.771116Z","iopub.execute_input":"2025-03-14T20:08:42.771392Z","iopub.status.idle":"2025-03-14T20:08:43.586622Z","shell.execute_reply.started":"2025-03-14T20:08:42.771371Z","shell.execute_reply":"2025-03-14T20:08:43.585685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get unique genres\nunique_genres = df[\"primary_genre\"].unique()\n# We create mapping for string labels to ints\ngenre_to_idx  = {genre: idx for idx, genre in enumerate(unique_genres)}\nidx_to_genre  = {idx: genre for genre, idx in genre_to_idx.items()}\n\nnum_classes = len(genre_to_idx)\nprint(\"Discovered classes:\", genre_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:44.565513Z","iopub.execute_input":"2025-03-14T20:08:44.565842Z","iopub.status.idle":"2025-03-14T20:08:44.578751Z","shell.execute_reply.started":"2025-03-14T20:08:44.565814Z","shell.execute_reply":"2025-03-14T20:08:44.578061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset class creation","metadata":{}},{"cell_type":"code","source":"class SheetMusicImageDataset(Dataset):\n    \"\"\"\n    Reads a CSV with columns: 'primary_genre', 'pdf' (originally).\n    Now, it loads the corresponding PNG directly from a folder.\n    \"\"\"\n    def __init__(self, csv_path, png_folder, transform=None, label_map=None):\n        self.df = pd.read_csv(csv_path)\n        self.png_folder = png_folder\n        self.transform = transform\n        self.label_map = label_map\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # Extract label\n        genre_str  = row[\"primary_genre\"]\n        label      = self.label_map[genre_str]  # integer label\n\n        # Extract the PNG filename (previously PDF)\n        pdf_filename = row[\"pdf\"]\n        png_filename = os.path.basename(pdf_filename).replace(\".pdf\", \".png\") # Convert PDF name to PNG\n\n        png_path = os.path.join(self.png_folder, png_filename)\n\n        # ✅ Load PNG Image Directly (No More PDF Conversion)\n        try:\n            image = Image.open(png_path).convert(\"RGB\")  # Open PNG instead of converting PDF\n        except Exception as e:\n            raise RuntimeError(f\"Error reading PNG {png_path}: {e}\")\n\n        # ✅ Apply transformations (resize, tensor conversion, etc.)\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:44.844602Z","iopub.execute_input":"2025-03-14T20:08:44.844890Z","iopub.status.idle":"2025-03-14T20:08:44.851142Z","shell.execute_reply.started":"2025-03-14T20:08:44.844866Z","shell.execute_reply":"2025-03-14T20:08:44.850115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformations for images","metadata":{}},{"cell_type":"code","source":"# We'll define separate transforms for train vs val if we want data augmentation in training.\nimport torchvision.transforms as T\n\ntrain_transform = T.Compose([\n    T.Lambda(lambda img: img.convert(\"RGB\")),  # ✅ Ensure images are RGB (3 channels)\n    T.Resize(IMG_SIZE),\n    T.RandomRotation(degrees=5),  # Small rotation only\n    T.ToTensor(),\n])\n\nval_transform = T.Compose([\n    T.Lambda(lambda img: img.convert(\"RGB\")),  # ✅ Ensure images are RGB\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n])\n\ntest_transform = T.Compose([\n    T.Lambda(lambda img: img.convert(\"RGB\")),  # ✅ Ensure images are RGB\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:46.714119Z","iopub.execute_input":"2025-03-14T20:08:46.714499Z","iopub.status.idle":"2025-03-14T20:08:46.720112Z","shell.execute_reply.started":"2025-03-14T20:08:46.714472Z","shell.execute_reply":"2025-03-14T20:08:46.719084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset creation","metadata":{}},{"cell_type":"code","source":"full_dataset = SheetMusicImageDataset(\n    csv_path=CSV_FILE,\n    png_folder=PDF_FOLDER,\n    transform=None,  # We'll set transforms after splitting\n    label_map=genre_to_idx\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:48.411178Z","iopub.execute_input":"2025-03-14T20:08:48.411457Z","iopub.status.idle":"2025-03-14T20:08:48.946288Z","shell.execute_reply.started":"2025-03-14T20:08:48.411435Z","shell.execute_reply":"2025-03-14T20:08:48.945298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split and loading","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\n# Define split sizes\ndataset_length = len(full_dataset)  # 33,000\ntrain_size = int(0.8 * dataset_length)  # 80% Train\nval_size = int(0.1 * dataset_length)  # 10% Validation\ntest_size = dataset_length - train_size - val_size  # Remaining for Test\n\n# Split dataset\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size], generator = g)\n\n# Assign transforms to each subset\ntrain_dataset.dataset.transform = train_transform\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = test_transform  # Define this if needed\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, generator=g)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n# Print stats\nprint(f\"Total samples: {dataset_length}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n# STRATIFIED SPLIT TODO !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:49.084992Z","iopub.execute_input":"2025-03-14T20:08:49.085431Z","iopub.status.idle":"2025-03-14T20:08:49.118995Z","shell.execute_reply.started":"2025-03-14T20:08:49.085401Z","shell.execute_reply":"2025-03-14T20:08:49.118272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization to check loading (sanity check)","metadata":{}},{"cell_type":"code","source":"# # Pick a random index for a random sample\n# random_idx = random.randint(0, len(full_dataset) - 1)\n\n# # Retrieve the sample\n# sample_img, sample_label = full_dataset[random_idx]\n\n# # We apply val_transform for display if not applied, just to ensure consistent sizing\n# if isinstance(sample_img, Image.Image):\n#     sample_img = val_transform(sample_img)\n\n# # Map label back to string\n# genre_name = idx_to_genre[sample_label] if sample_label in idx_to_genre else sample_label\n\n# # Plot\n# plt.figure(figsize=(6,6))             \n# plt.imshow(sample_img.squeeze(0), cmap='gray')\n# plt.title(f\"Random Loaded Sample\\nLabel: {genre_name}\", fontsize=14)\n# plt.axis('off')                      \n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:51.441335Z","iopub.execute_input":"2025-03-14T20:08:51.441621Z","iopub.status.idle":"2025-03-14T20:08:51.445104Z","shell.execute_reply.started":"2025-03-14T20:08:51.441600Z","shell.execute_reply":"2025-03-14T20:08:51.444309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torchvision.models as models\n# from tqdm import tqdm\n# import os\n\n# # Device setup\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Hyperparameters\n# learning_rate = 0.0005\n# epochs = 20\n# BATCH_SIZE = 64\n# NUM_CLASSES = len(genre_to_idx)\n\n# # Model configurations\n# model_names = [\"resnet50\", \"efficientnet_b0\", \"mobilenet_v3_small\"]\n# model_constructors = {\n#     \"resnet50\": models.resnet50,\n#     \"efficientnet_b0\": models.efficientnet_b0,\n#     \"mobilenet_v3_small\": models.mobilenet_v3_small\n# }\n\n# os.makedirs(\"saved_models\", exist_ok=True)\n\n# for model_name in model_names:\n#     print(f\"\\nTraining {model_name}...\")\n\n#     model = model_constructors[model_name](pretrained=True)\n\n#     for param in model.parameters():\n#         param.requires_grad = False\n\n#     # Adjust the output layer\n#     if \"resnet\" in model_name:\n#         model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n#     elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n#         model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n#     model = model.to(device)\n\n#     criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n#     for epoch in range(epochs):\n#         model.train()\n#         running_loss = 0.0\n\n#         with tqdm(total=len(train_loader), desc=f\"{model_name} Epoch {epoch+1}/{epochs}\") as pbar:\n#             for images, labels in train_loader:\n#                 images, labels = images.to(device), labels.to(device)\n#                 optimizer.zero_grad()\n#                 outputs = model(images)\n#                 loss = criterion(outputs, labels)\n#                 loss.backward()\n#                 optimizer.step()\n\n#                 running_loss += loss.item()\n#                 pbar.update(1)\n\n#         print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n#     # Save trained model\n#     save_path = f\"saved_models/{model_name}_model.pth\"\n#     torch.save(model.state_dict(), save_path)\n#     print(f\"✅ Saved {model_name} to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:53.181915Z","iopub.execute_input":"2025-03-14T20:08:53.182261Z","iopub.status.idle":"2025-03-14T20:08:53.186245Z","shell.execute_reply.started":"2025-03-14T20:08:53.182233Z","shell.execute_reply":"2025-03-14T20:08:53.185382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_names = [\"resnet50\", \"efficientnet_b0\", \"mobilenet_v3_small\"]\nmodel_constructors = {\n    \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\nall_results = {}\n\nfor model_name in model_names:\n    model = model_constructors[model_name](pretrained=False)\n\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model.load_state_dict(torch.load(f\"/kaggle/input/musicsheets/{model_name}_model.pth\"))\n    model = model.to(device)\n\n    model.eval()\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    all_results[model_name] = {\n        \"val_loss\": avg_val_loss,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    }\n\n# Save results to CSV\nresults_df = pd.DataFrame(all_results).T\nresults_df.to_csv(\"final_model_evaluation.csv\")\nprint(results_df)\n\n# Plot Loss, F1 Score, and Accuracy\nfig, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n# Validation Loss Plot\nresults_df['val_loss'].plot(kind='bar', ax=axes[0], color='salmon')\naxes[0].set_title(\"Validation Loss\")\naxes[0].set_ylabel(\"Loss\")\n\n# F1 Score Plot\nresults_df['val_f1'].plot(kind='bar', ax=axes[1], color='skyblue')\naxes[1].set_title('F1 Score')\naxes[1].set_ylabel(\"F1 Score\")\n\n# Accuracy Plot\nresults_df['val_accuracy'].plot(kind='bar', ax=axes[2], color='lightgreen')\naxes[2].set_title(\"Accuracy\")\naxes[2].set_ylabel(\"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"final_model_metrics.png\")\nplt.show()\n\nprint(\"✅ Final results and plots saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T20:08:55.587504Z","iopub.execute_input":"2025-03-14T20:08:55.587831Z","iopub.status.idle":"2025-03-14T20:10:22.594968Z","shell.execute_reply.started":"2025-03-14T20:08:55.587802Z","shell.execute_reply":"2025-03-14T20:10:22.593692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Convert desired PDFs to PNG images","metadata":{}},{"cell_type":"code","source":"# import os\n# from pdf2image import convert_from_path\n\n# # Paths\n# PDF_DIR = \"archives/pdf/\"  # Folder containing PDFs\n# IMAGE_DIR = \"preprocessed_images/\"  # Folder to store preprocessed images\n\n# import os\n# import pandas as pd\n# from pdf2image import convert_from_path\n\n# # Paths\n# PDF_DIR = \"archives/pdf/\"  # Folder containing PDFs\n# IMAGE_DIR = \"preprocessed_images/\"  # Folder to store preprocessed images\n# CSV_PATH = \"dataset.csv\"  # CSV file with PDF paths\n\n# # Ensure output directory exists\n# os.makedirs(IMAGE_DIR, exist_ok=True)\n\n# # Load dataset CSV\n# df = pd.read_csv(CSV_PATH)\n\n# # ✅ Normalize filenames (strip spaces, remove paths)\n# required_pdfs = set(df[\"pdf\"].str.strip().apply(lambda x: os.path.basename(x)))  # Only filename (no path)\n\n# print(\"Preprocessing PDFs into images...\")\n\n# for root, _, files in os.walk(PDF_DIR):\n#     for pdf_file in files:\n#         pdf_file_cleaned = pdf_file.strip()  # Ensure no extra spaces\n#         if pdf_file_cleaned.endswith(\".pdf\") and pdf_file_cleaned in required_pdfs:  # ✅ Corrected filename comparison\n#             pdf_path = os.path.join(root, pdf_file_cleaned)\n#             images = convert_from_path(pdf_path)  # Convert PDF to images\n            \n#             # Save first page only\n#             img_save_path = os.path.join(IMAGE_DIR, pdf_file_cleaned.replace(\".pdf\", \".png\"))\n#             images[0].save(img_save_path, \"PNG\")  # Save first page as image\n#             print(f\"✅ Saved: {img_save_path}\")\n\n# print(\"✅ PDF to PNG conversion complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Definition (change)","metadata":{}},{"cell_type":"code","source":"# class SimpleCNN(nn.Module): \n#     def __init__(self, num_classes):\n#         super(SimpleCNN, self).__init__()\n#         # Input shape: (1, 512, 512) if using Grayscale\n#         self.net = nn.Sequential(\n#             nn.Conv2d(1, 16, kernel_size=3, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),  # -> (16, 256, 256)\n            \n#             nn.Conv2d(16, 32, kernel_size=3, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),  # -> (32, 128, 128)\n            \n#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),  # -> (64, 64, 64)\n#         )\n        \n#         # After 3 pool layers, each dimension is divided by 2^3 = 8\n#         # So for a 512×512 input, we now have 64×64×64 features\n#         # Flatten shape is 64 * 64 * 64 = 262144\n#         self.fc = nn.Sequential(\n#             nn.Flatten(),\n#             nn.Linear(64 * (IMG_SIZE[0]//8) * (IMG_SIZE[1]//8), 128),\n#             nn.ReLU(),\n#             nn.Linear(128, num_classes)\n#         )\n\n#     def forward(self, x):\n#         x = self.net(x)\n#         x = self.fc(x)\n#         return x\n\n# model = SimpleCNN(num_classes=num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### Training setup","metadata":{}},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print('Using:', device)\n# model.to(device)\n\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n\n# # Use ReduceLROnPlateau to reduce LR if val_loss stops improving.\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n\n# # Early stopping variables\n# best_val_loss = float(\"inf\")\n# best_model_wts = copy.deepcopy(model.state_dict())\n# no_improve_count = 0\n\n# # Lists to track metrics for plotting\n# train_losses = []\n# val_losses   = []\n# train_accs   = []\n# val_accs     = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"# for epoch in range(NUM_EPOCHS):\n#     # --- TRAIN ---\n#     model.train()\n#     running_loss, running_correct, total = 0.0, 0, 0\n    \n#     for images, labels in train_loader:\n#         images, labels = images.to(device), labels.to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(images)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         # stats\n#         running_loss += loss.item() * images.size(0)\n#         _, predicted = torch.max(outputs, 1)\n#         running_correct += (predicted == labels).sum().item()\n#         total += labels.size(0)\n    \n#     train_epoch_loss = running_loss / total\n#     train_epoch_acc  = running_correct / total\n#     train_losses.append(train_epoch_loss)\n#     train_accs.append(train_epoch_acc)\n\n#     # --- VALIDATION ---\n#     model.eval()\n#     val_running_loss, val_correct, val_total = 0.0, 0, 0\n    \n#     with torch.no_grad():\n#         for images, labels in val_loader:\n#             images, labels = images.to(device), labels.to(device)\n#             outputs = model(images)\n#             loss = criterion(outputs, labels)\n\n#             val_running_loss += loss.item() * images.size(0)\n#             _, predicted = torch.max(outputs, 1)\n#             val_correct += (predicted == labels).sum().item()\n#             val_total += labels.size(0)\n\n#     val_epoch_loss = val_running_loss / val_total\n#     val_epoch_acc  = val_correct / val_total\n#     val_losses.append(val_epoch_loss)\n#     val_accs.append(val_epoch_acc)\n\n#     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n#           f\"Train Loss: {train_epoch_loss:.4f} | Train Acc: {train_epoch_acc:.4f} | \"\n#           f\"Val Loss: {val_epoch_loss:.4f}   | Val Acc: {val_epoch_acc:.4f}\")\n\n#     # We'll step the scheduler based on validation loss\n#     scheduler.step(val_epoch_loss)\n\n#     # --- EARLY STOPPING CHECK ---\n#     if val_epoch_loss < best_val_loss:\n#         best_val_loss = val_epoch_loss\n#         best_model_wts = copy.deepcopy(model.state_dict())\n#         no_improve_count = 0\n#     else:\n#         no_improve_count += 1\n#         print(f\"No improvement for {no_improve_count} epoch(s)\")\n\n#         if no_improve_count >= EARLY_STOP_PATIENCE:\n#             print(\"Early stopping triggered!\")\n#             break\n\n# print(\"Training complete!\")\n\n# # Load best weights\n# model.load_state_dict(best_model_wts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plot curves","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(12,5))\n\n# # --- LOSS ---\n# plt.subplot(1,2,1)\n# plt.plot(train_losses, label='Train Loss', marker='o')\n# plt.plot(val_losses,   label='Val Loss', marker='o')\n# plt.title(\"Loss Over Epochs\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Loss\")\n# plt.legend()\n\n# # --- ACCURACY ---\n# plt.subplot(1,2,2)\n# plt.plot(train_accs, label='Train Acc', marker='o')\n# plt.plot(val_accs,   label='Val Acc', marker='o')\n# plt.title(\"Accuracy Over Epochs\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Accuracy\")\n# plt.legend()\n\n# plt.tight_layout","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}