{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11175666,"sourceType":"datasetVersion","datasetId":6847296},{"sourceId":11190260,"sourceType":"datasetVersion","datasetId":6862038}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":16321.680441,"end_time":"2025-03-25T05:02:39.748753","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-25T00:30:38.068312","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Libraries for all cells","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport copy\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as T\nimport torchvision.models as models\nfrom tqdm import tqdm\nimport re\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom pdf2image import convert_from_path\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:07:15.625939Z","iopub.execute_input":"2025-03-28T05:07:15.626245Z","iopub.status.idle":"2025-03-28T05:07:18.961021Z","shell.execute_reply.started":"2025-03-28T05:07:15.626224Z","shell.execute_reply":"2025-03-28T05:07:18.960233Z"},"papermill":{"duration":8.217866,"end_time":"2025-03-25T00:30:48.828473","exception":false,"start_time":"2025-03-25T00:30:40.610607","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"CSV_FILE       = \"/kaggle/input/musicsheets/preprocessed_images/dataset.csv\"        # csv with 'primary_genre' and 'pdf' columns\nPDF_FOLDER     = \"/kaggle/input/musicsheets/preprocessed_images/preprocessed_images/\"       # folder containing PDF files\nIMG_SIZE       = (512, 512)\nBATCH_SIZE     = 32\nVAL_SPLIT      = 0.1\nNUM_EPOCHS     = 1\nLEARNING_RATE  = 0.0005\nRANDOM_SEED    = 42\nUNFROZEN_LAYERS = 3\ntorch.manual_seed(RANDOM_SEED)\ng = torch.Generator() # for DataLoader shuffling\ng.manual_seed(42)  # for same splits of data","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:07:56.442079Z","iopub.execute_input":"2025-03-28T05:07:56.442389Z","iopub.status.idle":"2025-03-28T05:07:56.450962Z","shell.execute_reply.started":"2025-03-28T05:07:56.442367Z","shell.execute_reply":"2025-03-28T05:07:56.449976Z"},"papermill":{"duration":0.014789,"end_time":"2025-03-25T00:30:48.848840","exception":false,"start_time":"2025-03-25T00:30:48.834051","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7cfd49caf750>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# 2. Data loading pipeline","metadata":{"papermill":{"duration":0.004597,"end_time":"2025-03-25T00:30:48.872661","exception":false,"start_time":"2025-03-25T00:30:48.868064","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.1 Read CSV","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(CSV_FILE)","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:07:59.198761Z","iopub.execute_input":"2025-03-28T05:07:59.199071Z","iopub.status.idle":"2025-03-28T05:07:59.784529Z","shell.execute_reply.started":"2025-03-28T05:07:59.199050Z","shell.execute_reply":"2025-03-28T05:07:59.783557Z"},"papermill":{"duration":0.815746,"end_time":"2025-03-25T00:30:49.693203","exception":false,"start_time":"2025-03-25T00:30:48.877457","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Get unique genres\nunique_genres = df[\"primary_genre\"].unique()\n# We create mapping for string labels to ints\ngenre_to_idx  = {genre: idx for idx, genre in enumerate(unique_genres)}\nidx_to_genre  = {idx: genre for genre, idx in genre_to_idx.items()}\n\nnum_classes = len(genre_to_idx)\nprint(\"Discovered classes:\", genre_to_idx)","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:07:59.785802Z","iopub.execute_input":"2025-03-28T05:07:59.786138Z","iopub.status.idle":"2025-03-28T05:07:59.794148Z","shell.execute_reply.started":"2025-03-28T05:07:59.786105Z","shell.execute_reply":"2025-03-28T05:07:59.793284Z"},"papermill":{"duration":0.019969,"end_time":"2025-03-25T00:30:49.718425","exception":false,"start_time":"2025-03-25T00:30:49.698456","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Discovered classes: {'Classical': 0, 'Electronic & Dance': 1, 'Folk/World': 2, 'Pop': 3, 'R&B, Soul & Hip-Hop': 4, 'Religious': 5, 'Rock & Metal': 6, 'Soundtrack': 7}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### 2.2 PDF to PNG","metadata":{}},{"cell_type":"code","source":"# IMAGE_DIR = \"/kaggle/working/preprocessed_imagess/\"  # folder to store preprocessed images\n\n# # Ensure output directory exists\n# os.makedirs(IMAGE_DIR, exist_ok=True)\n\n# required_pdfs = set(df[\"pdf\"].str.strip().apply(lambda x: os.path.basename(x)))  # only filename (no path)\n# print(\"Preprocessing PDFs into images...\")\n\n# for root, _, files in os.walk(PDF_FOLDER):\n#     for pdf_file in files:\n#         pdf_file_cleaned = pdf_file.strip()  # no extra spaces\n#         if pdf_file_cleaned.endswith(\".pdf\") and pdf_file_cleaned in required_pdfs:  # file in pdf folder but also has path in csv\n#             pdf_path = os.path.join(root, pdf_file_cleaned)\n#             images = convert_from_path(pdf_path)  # convert \n#             img_save_path = os.path.join(IMAGE_DIR, pdf_file_cleaned.replace(\".pdf\", \".png\"))\n#             images[0].save(img_save_path, \"PNG\")\n#             print(f\"Saved: {img_save_path}\")\n\n# print(\"PDF to PNG conversion complete!!!!!!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 Dataset class creation","metadata":{"papermill":{"duration":0.00499,"end_time":"2025-03-25T00:30:49.728607","exception":false,"start_time":"2025-03-25T00:30:49.723617","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class SheetMusicImageDataset(Dataset):\n    \"\"\"\n    Reads a CSV with columns: 'primary_genre', 'pdf' (originally).\n    Now, it loads the corresponding PNG directly from a folder.\n    \"\"\"\n    def __init__(self, csv_path, png_folder, transform=None, label_map=None):\n        self.df = pd.read_csv(csv_path)\n        self.png_folder = png_folder\n        self.transform = transform\n        self.label_map = label_map\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # Extract label\n        genre_str  = row[\"primary_genre\"]\n        label      = self.label_map[genre_str]  # integer label\n\n        # Extract the png filename\n        pdf_filename = row[\"pdf\"] # pdf name in csv\n        png_filename = os.path.basename(pdf_filename).replace(\".pdf\", \".png\") # convert pdf name to png\n\n        png_path = os.path.join(self.png_folder, png_filename)\n        try:\n            image = Image.open(png_path).convert(\"RGB\")  # open PNG instead of converting PDF\n        except Exception as e:\n            raise RuntimeError(f\"Error reading PNG {png_path}: {e}\")\n\n        if self.transform: # if transformation not none apply it\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:08:02.675839Z","iopub.execute_input":"2025-03-28T05:08:02.676166Z","iopub.status.idle":"2025-03-28T05:08:02.682299Z","shell.execute_reply.started":"2025-03-28T05:08:02.676138Z","shell.execute_reply":"2025-03-28T05:08:02.681385Z"},"papermill":{"duration":0.012282,"end_time":"2025-03-25T00:30:49.746509","exception":false,"start_time":"2025-03-25T00:30:49.734227","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 2.4 Transformations for images","metadata":{"papermill":{"duration":0.00475,"end_time":"2025-03-25T00:30:49.756124","exception":false,"start_time":"2025-03-25T00:30:49.751374","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# We'll define separate transforms for train vs val if we want data augmentation in training.\ntrain_transform = T.Compose([\n    T.Lambda(lambda img: img.convert(\"RGB\")),  # üîÑ Ensure all images have 3 channels\n    T.Resize(IMG_SIZE),            # üìè Resize to same shape\n    T.RandomAffine(degrees=3, translate=(0.03, 0.03), scale=(0.95, 1.05)),  # slight warping\n    T.RandomPerspective(distortion_scale=0.2, p=0.2), # distortions\n    T.ColorJitter(brightness=0.1, contrast=0.1), # lighting/contrast changes\n    T.RandomInvert(p=0.1), # negative-style variations\n    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)), # blur effects\n    T.ToTensor(),\n    T.Normalize(mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225)),\n])\n\n\n## train_transform = T.Compose([\n#     T.Lambda(lambda img: img.convert(\"RGB\")),  # ensure images are RGB \n#     T.Resize(IMG_SIZE),\n#     T.RandomRotation(degrees=5),  # Small rotation only\n#     T.ToTensor(),\n# ])\n\nval_transform = T.Compose([\n    T.Lambda(lambda img: img.convert(\"RGB\")),  # ensure images are RGB\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n])\n\ntest_transform = T.Compose([\n    T.Lambda(lambda img: img.convert(\"RGB\")),  # ensure images are RGB\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n])\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:08:05.227412Z","iopub.execute_input":"2025-03-28T05:08:05.227770Z","iopub.status.idle":"2025-03-28T05:08:05.234651Z","shell.execute_reply.started":"2025-03-28T05:08:05.227745Z","shell.execute_reply":"2025-03-28T05:08:05.233643Z"},"papermill":{"duration":1.920101,"end_time":"2025-03-25T00:30:51.681119","exception":false,"start_time":"2025-03-25T00:30:49.761018","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 2.5 Dataset creation","metadata":{"papermill":{"duration":0.004829,"end_time":"2025-03-25T00:30:51.692283","exception":false,"start_time":"2025-03-25T00:30:51.687454","status":"completed"},"tags":[]}},{"cell_type":"code","source":"full_dataset = SheetMusicImageDataset(\n    csv_path=CSV_FILE,\n    png_folder=PDF_FOLDER,\n    transform=None,  # We'll set transforms after splitting\n    label_map=genre_to_idx\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:08:07.748328Z","iopub.execute_input":"2025-03-28T05:08:07.748651Z","iopub.status.idle":"2025-03-28T05:08:08.309403Z","shell.execute_reply.started":"2025-03-28T05:08:07.748627Z","shell.execute_reply":"2025-03-28T05:08:08.308683Z"},"papermill":{"duration":0.584756,"end_time":"2025-03-25T00:30:52.281934","exception":false,"start_time":"2025-03-25T00:30:51.697178","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 2.6 Split and loading","metadata":{"papermill":{"duration":0.004949,"end_time":"2025-03-25T00:30:52.292363","exception":false,"start_time":"2025-03-25T00:30:52.287414","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\n# Define split sizes\ndataset_length = len(full_dataset)  # 33,000\ntrain_size = int((1-2*VAL_SPLIT) * dataset_length)  # 80% Train\nval_size = int(VAL_SPLIT * dataset_length)  # 10% Validation\ntest_size = dataset_length - train_size - val_size  # Remaining for Test\n\n# Split dataset\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size], generator = g)\n\n# Assign transforms to each subset\ntrain_dataset.dataset.transform = train_transform\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = test_transform \n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, generator=g)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n# Print stats\nprint(f\"Total samples: {dataset_length}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T05:08:08.310610Z","iopub.execute_input":"2025-03-28T05:08:08.310867Z","iopub.status.idle":"2025-03-28T05:08:08.339305Z","shell.execute_reply.started":"2025-03-28T05:08:08.310847Z","shell.execute_reply":"2025-03-28T05:08:08.338353Z"},"papermill":{"duration":0.038341,"end_time":"2025-03-25T00:30:52.335793","exception":false,"start_time":"2025-03-25T00:30:52.297452","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Total samples: 32822, Train: 26257, Val: 3282, Test: 3283\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# 3. Model comparison","metadata":{}},{"cell_type":"markdown","source":"#### Run Section 1 & 2","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom tqdm import tqdm\nimport os\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nNUM_CLASSES = len(genre_to_idx)\n\n# Pretrained models on ImageNet\nmodel_names = [\"resnet50\", \"efficientnet_b0\", \"mobilenet_v3_small\"]\nmodel_constructors = {\n    \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor model_name in model_names:\n    print(f\"\\nTraining {model_name}...\")\n\n    model = model_constructors[model_name](pretrained=True) # weights learned from ImageNet\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Adjust the output layer\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        with tqdm(total=len(train_loader), desc=f\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\") as pbar:\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n                pbar.update(1)\n\n        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n    # Save trained model\n    save_path = f\"saved_models/{model_name}_model.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Saved {model_name} to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:08:11.512679Z","iopub.execute_input":"2025-03-28T05:08:11.512979Z","iopub.status.idle":"2025-03-28T05:08:25.412553Z","shell.execute_reply.started":"2025-03-28T05:08:11.512960Z","shell.execute_reply":"2025-03-28T05:08:25.411045Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTraining resnet50...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 192MB/s]\nresnet50 Epoch 1/1:   1%|          | 8/821 [00:12<20:54,  1.54s/it]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-fbc0eadb63db>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"### 3.2 Evaluation of models","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_names = [\"resnet50\", \"efficientnet_b0\", \"mobilenet_v3_small\"]\nmodel_constructors = {\n    \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\nall_results = {}\n\nfor model_name in model_names:\n    model = model_constructors[model_name](pretrained=False)\n\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model.load_state_dict(torch.load(f\"/kaggle/input/musicsheets/{model_name}_model.pth\"))\n    model = model.to(device)\n\n    model.eval()\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    all_results[model_name] = {\n        \"val_loss\": avg_val_loss,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    }\n\n# Save results to a csv\nresults_df = pd.DataFrame(all_results).T\nresults_df.to_csv(\"final_model_evaluation.csv\")\nprint(results_df)\n\n# Plot loss, F1 Score, and accuracy\nfig, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n# Loss \nresults_df['val_loss'].plot(kind='bar', ax=axes[0], color='salmon')\naxes[0].set_title(\"Validation Loss\")\naxes[0].set_ylabel(\"Loss\")\n\n# F1 score\nresults_df['val_f1'].plot(kind='bar', ax=axes[1], color='skyblue')\naxes[1].set_title('F1 Score')\naxes[1].set_ylabel(\"F1 Score\")\n\n# Accuracy\nresults_df['val_accuracy'].plot(kind='bar', ax=axes[2], color='lightgreen')\naxes[2].set_title(\"Accuracy\")\naxes[2].set_ylabel(\"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"final_model_metrics.png\")\nplt.show()\n\nprint(\"Final results and plots saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:08:30.306098Z","iopub.execute_input":"2025-03-28T05:08:30.306480Z","iopub.status.idle":"2025-03-28T05:13:43.799814Z","shell.execute_reply.started":"2025-03-28T05:08:30.306434Z","shell.execute_reply":"2025-03-28T05:13:43.798591Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n<ipython-input-12-fd2aead40463>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"/kaggle/input/musicsheets/{model_name}_model.pth\"))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n<ipython-input-12-fd2aead40463>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"/kaggle/input/musicsheets/{model_name}_model.pth\"))\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n<ipython-input-12-fd2aead40463>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"/kaggle/input/musicsheets/{model_name}_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"                    val_loss  val_precision  val_recall    val_f1  \\\nresnet50            1.324382       0.414932    0.254973  0.224875   \nefficientnet_b0     1.179676       0.381876    0.271069  0.254915   \nmobilenet_v3_small  1.234477       0.360274    0.233484  0.227850   \n\n                    val_accuracy  \nresnet50                0.559110  \nefficientnet_b0         0.600548  \nmobilenet_v3_small      0.502438  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x500 with 3 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Final results and plots saved\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 4. Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Learning Rate Tuning","metadata":{}},{"cell_type":"markdown","source":"#### Run Section 1 & 2 and adjust LEARNING_RATE","metadata":{}},{"cell_type":"markdown","source":"### 4.1.1 Training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom tqdm import tqdm\nimport os\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Change parameters at the top of the notebook and run this cell\n\n# Model configurations\nmodel_names = [\"efficientnet_b0\"]\nmodel_constructors = {\n    # \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    # \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor model_name in model_names:\n    print(f\"\\nTraining {model_name}...\")\n\n    model = model_constructors[model_name](pretrained=True)\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Unfreeze convolutional layers\n    for layer in model.features[-UNFROZEN_LAYERS:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n\n\n    # Adjust the output layer\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        with tqdm(total=len(train_loader), desc=f\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\") as pbar:\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                pbar.update(1)\n\n        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n    # Save trained model\n    save_path = f\"saved_models/{model_name}_{LEARNING_RATE}_model_{UNFROZEN_LAYERS}.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Saved {model_name} to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:25:41.800187Z","iopub.execute_input":"2025-03-28T05:25:41.800612Z","iopub.status.idle":"2025-03-28T05:25:54.226666Z","shell.execute_reply.started":"2025-03-28T05:25:41.800582Z","shell.execute_reply":"2025-03-28T05:25:54.225207Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n\nTraining efficientnet_b0...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 87.1MB/s]\nefficientnet_b0 Epoch 1/1:   1%|          | 8/821 [00:11<20:03,  1.48s/it]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-3e2d1b82437e>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"### 4.1.2 Comparison of models with different hyperparameter values","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\n# Path to your saved models\nmodel_folder = \"/kaggle/input/hyper/\"\n\n# Get all model paths\nmodel_files = sorted([f for f in os.listdir(model_folder) if f.endswith('.pth')])\n\n# Results dictionary\nall_results = {}\n\nfor file_name in model_files:\n    lr_match = re.search(r'b0_(\\d\\.\\d+)_model', file_name)\n    lr = lr_match.group(1) if lr_match else 'unknown'\n\n    unfrozen_layers = 1 if \"_1.pth\" in file_name else 0\n    model_key = f\"LR_{lr}_Unfrozen_{unfrozen_layers}\"\n\n    # Load EfficientNet B0\n    model = models.efficientnet_b0(pretrained=False)\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model.load_state_dict(torch.load(os.path.join(model_folder, file_name)))\n    model = model.to(device)\n    model.eval()\n\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    all_results[model_key] = {\n        \"learning_rate\": float(lr),\n        \"unfrozen_layers\": unfrozen_layers,\n        \"val_loss\": avg_val_loss,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    }\n\n# Save results to csv\nresults_df = pd.DataFrame.from_dict(all_results, orient='index').sort_values(by=['learning_rate', 'unfrozen_layers'])\nresults_df.to_csv(\"hyperparameter_results.csv\", index=False)\nprint(results_df)\n\n# Plot metrics\nmetrics = ['val_loss', 'val_f1', 'val_accuracy']\ntitles = ['Validation Loss', 'Validation F1 Score', 'Validation Accuracy']\ncolors = ['salmon', 'skyblue', 'lightgreen']\n\nfig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx]\n    for unfrozen in [0, 1]:\n        subset = results_df[results_df['unfrozen_layers'] == unfrozen]\n        linestyle = '--' if unfrozen == 1 else '-'\n        label = '1 Unfrozen Layer' if unfrozen == 1 else 'Output Only'\n        ax.plot(subset['learning_rate'], subset[metric], linestyle=linestyle, marker='o', color=colors[idx], label=label)\n\n    ax.set_xlabel(\"Learning Rate\")\n    ax.set_title(titles[idx])\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.savefig(\"hyperparameter_comparison.png\")\nplt.show()\n\nprint(\"Hyperparameter evaluation done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T05:25:58.870223Z","iopub.execute_input":"2025-03-28T05:25:58.870611Z","iopub.status.idle":"2025-03-28T05:25:58.894212Z","shell.execute_reply.started":"2025-03-28T05:25:58.870580Z","shell.execute_reply":"2025-03-28T05:25:58.893033Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a0cc99df5295>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get all model paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Results dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/hyper/'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/hyper/'","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"## 4.2 Unfrozen Layers Tuning","metadata":{}},{"cell_type":"markdown","source":"#### Run Section 1 & 2 and adjust UNFROZEN_LAYERS","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1 Training","metadata":{}},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nNUM_CLASSES = len(genre_to_idx)\n\n# Model configurations\nmodel_names = [\"efficientnet_b0\"]\nmodel_constructors = {\n    # \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    # \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor model_name in model_names:\n    print(f\"\\nTraining {model_name}...\")\n\n    model = model_constructors[model_name](pretrained=True)\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Unfreeze convolutional layers\n    for layer in model.features[-UNFROZEN_LAYERS:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n\n\n    # Adjust the output layer\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        with tqdm(total=len(train_loader), desc=f\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\") as pbar:\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                pbar.update(1)\n\n        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n    # Save trained model\n    save_path = f\"saved_models/{model_name}_{LEARNING_RATE}_model_{UNFROZEN_LAYERS}.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Saved {model_name} to {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2.2 Evaluation","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\n# Path to your saved models (exact Kaggle dataset path)\nmodel_folder = \"/kaggle/input/musicsheetsblanced/layers\"\n\n# Get all model files\nmodel_files = sorted([f for f in os.listdir(model_folder) if f.endswith('.pth')])\n\n# Store results\nall_results = {}\n\n# Loop through each model file\nfor file_name in model_files:\n    # Extract learning rate and unfrozen layers from filename\n    lr_match = re.search(r'b0_(\\d\\.\\d+)_model', file_name)\n    lr = lr_match.group(1) if lr_match else 'unknown'\n    \n    unfrozen_layers_match = re.search(r'model_(\\d+)\\.pth', file_name)\n    unfrozen_layers = int(unfrozen_layers_match.group(1)) if unfrozen_layers_match else 0\n    \n    model_key = f\"LR_{lr}_Unfrozen_{unfrozen_layers}\"\n    \n    # Load EfficientNet-B0\n    model = models.efficientnet_b0(pretrained=False)\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model.load_state_dict(torch.load(os.path.join(model_folder, file_name)))\n    model = model.to(device)\n    model.eval()\n\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    all_results[model_key] = {\n        \"learning_rate\": float(lr),\n        \"unfrozen_layers\": unfrozen_layers,\n        \"val_loss\": avg_val_loss,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    }\n\n# Save results to csv and print table\nresults_df = pd.DataFrame.from_dict(all_results, orient='index').sort_values(by=['unfrozen_layers'])\nresults_df.to_csv(\"layers_results.csv\", index=False)\nprint(\"Metrics Table:\")\nprint(results_df)\n\n# Plot each metric in a clear bar chart comparing unfrozen layers\nmetrics = ['val_loss', 'val_f1', 'val_accuracy']\ntitles = ['Validation Loss', 'Validation F1 Score', 'Validation Accuracy']\ncolors = ['salmon', 'skyblue', 'lightgreen']\n\nfig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx]\n    ax.bar(\n        results_df['unfrozen_layers'].astype(str),\n        results_df[metric],\n        color=colors[idx],\n        width=0.6\n    )\n    ax.set_xlabel(\"Number of Unfrozen Layers\")\n    ax.set_ylabel(metric)\n    ax.set_title(titles[idx])\n    ax.grid(axis='y')\n\nplt.tight_layout()\nplt.savefig(\"layers_comparison.png\")\nplt.show()\n\nprint(\"Hyperparameter evaluation done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3 Batch-size Tuning","metadata":{}},{"cell_type":"markdown","source":"#### Run Section 1 & 2 and adjust BATCH_SIZE","metadata":{}},{"cell_type":"markdown","source":"### 4.3.1 Training","metadata":{}},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nNUM_CLASSES = len(genre_to_idx)\n\n# Model configurations\nmodel_names = [\"efficientnet_b0\"]\nmodel_constructors = {\n    # \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    # \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor model_name in model_names:\n    print(f\"\\nTraining {model_name}...\")\n\n    model = model_constructors[model_name](pretrained=True)\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Unfreeze convolutional layers\n    for layer in model.features[-UNFROZEN_LAYERS:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n\n\n    # Adjust the output layer\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        with tqdm(total=len(train_loader), desc=f\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\") as pbar:\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                pbar.update(1)\n\n        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n    # Save trained model\n    save_path = f\"saved_models/{model_name}_{LEARNING_RATE}_model_{UNFROZEN_LAYERS}_{BATCH_SIZE}_aug.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Saved {model_name} to {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.2 Evaluation","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\n# Path to your saved models (adjusted to exact Kaggle dataset path)\nmodel_folder = \"/kaggle/input/musicsheetsblanced/batch\"\n\n# Get all model files\nmodel_files = sorted([f for f in os.listdir(model_folder) if f.endswith('.pth')])\n\n# Store results\nall_results = {}\n\n# Loop through each model file\nfor file_name in model_files:\n    # Extract batch size from filename\n    batch_size_match = re.search(r'model_\\d+_(\\d+)\\.pth', file_name)\n    batch_size = int(batch_size_match.group(1)) if batch_size_match else 'unknown'\n\n    model_key = f\"Batch_{batch_size}\"\n\n    # Load EfficientNet-B0\n    model = models.efficientnet_b0(pretrained=False)\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model.load_state_dict(torch.load(os.path.join(model_folder, file_name)))\n    model = model.to(device)\n    model.eval()\n\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    all_results[model_key] = {\n        \"batch_size\": batch_size,\n        \"val_loss\": avg_val_loss,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    }\n\n# Save results to CSV and print table\nresults_df = pd.DataFrame.from_dict(all_results, orient='index').sort_values(by=['batch_size'])\nresults_df.to_csv(\"batchsize_results.csv\", index=False)\nprint(\"Metrics Table by Batch Size:\")\nprint(results_df)\n\n# Plot each metric as line chart clearly comparing batch sizes\nmetrics = ['val_loss', 'val_f1', 'val_accuracy']\ntitles = ['Validation Loss by Batch Size', 'Validation F1 Score by Batch Size', 'Validation Accuracy by Batch Size']\ncolors = ['salmon', 'skyblue', 'lightgreen']\n\nfig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx]\n    ax.plot(\n        results_df['batch_size'],\n        results_df[metric],\n        marker='o',\n        linestyle='-',\n        color=colors[idx],\n        linewidth=2\n    )\n    ax.set_xlabel(\"Batch Size\")\n    ax.set_ylabel(metric)\n    ax.set_title(titles[idx])\n    ax.set_xticks(results_df['batch_size'])\n    ax.grid(True)\n\nplt.tight_layout()\nplt.savefig(\"batchsize_comparison.png\")\nplt.show()\n\nprint(\"Batch size evaluation and plots done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.4 Augmentation Experiment","metadata":{}},{"cell_type":"markdown","source":"#### Run Section 1 & 2 and uncomment the Train Tranformation with augmentations","metadata":{}},{"cell_type":"markdown","source":"### 4.4.1 Training","metadata":{}},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nNUM_CLASSES = len(genre_to_idx)\n\n# Model configurations\nmodel_names = [\"efficientnet_b0\"]\nmodel_constructors = {\n    # \"resnet50\": models.resnet50,\n    \"efficientnet_b0\": models.efficientnet_b0,\n    # \"mobilenet_v3_small\": models.mobilenet_v3_small\n}\n\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor model_name in model_names:\n    print(f\"\\nTraining {model_name}...\")\n\n    model = model_constructors[model_name](pretrained=True)\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    for layer in model.features[-UNFROZEN_LAYERS:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n\n\n    # Adjust the output layer\n    if \"resnet\" in model_name:\n        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n    elif \"efficientnet\" in model_name or \"mobilenet\" in model_name:\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        with tqdm(total=len(train_loader), desc=f\"{model_name} Epoch {epoch+1}/{NUM_EPOCHS}\") as pbar:\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                pbar.update(1)\n\n        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n\n    # Save trained model\n    save_path = f\"saved_models/{model_name}_{LEARNING_RATE}_model_{UNFROZEN_LAYERS}_{BATCH_SIZE}_aug.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Saved {model_name} to {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.4.2 Evaluation","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\n# Path to your saved models\nmodel_folder = \"/kaggle/input/musicsheetsblanced/aug\"\nmodel_files = sorted([f for f in os.listdir(model_folder) if f.endswith('.pth')])\n\n# Store results\nall_results = {}\n\nfor file_name in model_files:\n    # Determine if model was trained with augmentation\n    is_aug = \"_aug\" in file_name\n    model_label = \"With Augmentation\" if is_aug else \"No Augmentation\"\n\n    # Load model\n    model = models.efficientnet_b0(pretrained=False)\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n    model.load_state_dict(torch.load(os.path.join(model_folder, file_name)))\n    model = model.to(device)\n    model.eval()\n\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    all_results[model_label] = {\n        \"val_loss\": avg_val_loss,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    }\n\n# Create DataFrame and save table\nresults_df = pd.DataFrame.from_dict(all_results, orient='index')\nresults_df.to_csv(\"augmentation_comparison.csv\")\nprint(\"Results Table:\")\nprint(results_df)\n\n# Plot comparison for each metric\nmetrics = ['val_loss', 'val_f1', 'val_accuracy']\ntitles = ['Validation Loss', 'Validation F1 Score', 'Validation Accuracy']\ncolors = ['salmon', 'skyblue', 'lightgreen']\n\nfig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx]\n    ax.bar(results_df.index, results_df[metric], color=colors[idx], width=0.6)\n    ax.set_title(titles[idx])\n    ax.set_ylabel(metric.replace(\"val_\", \"\").capitalize())\n    ax.set_xlabel(\"Model\")\n    ax.grid(axis='y')\n\nplt.tight_layout()\nplt.savefig(\"augmentation_effect_comparison.png\")\nplt.show()\n\nprint(\"Plots comparing augmentation vs no augmentation done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Resume Learning for a model","metadata":{}},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Training config\nNUM_CLASSES = len(genre_to_idx)\n\n# Load EfficientNet-B0 model (the best of the three)\nmodel = models.efficientnet_b0(pretrained=False)\n\n# Adjust classifier\nmodel.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n\n# Load previously saved weights\nmodel_path = \"/kaggle/input/musicsheets/efficientnet_b0_0.0001_model_3_32_aug_150ep.pth\"\nmodel.load_state_dict(torch.load(model_path, map_location=device))\n\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze last 3 convolutional layers\nfor layer in model.features[-3:]:\n    for param in layer.parameters():\n        param.requires_grad = True\n\n# Ensure classifier is trainable\nfor param in model.classifier[-1].parameters():\n    param.requires_grad = True\n\nmodel = model.to(device)\n\n# Define loss and optimizer (only trainable params)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n\n    with tqdm(total=len(train_loader), desc=f\"Continue Epoch {epoch+1}/{NUM_EPOCHS}\") as pbar:\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            pbar.update(1)\n\n    print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader):.4f}\")\n\n# Save model\nos.makedirs(\"saved_models\", exist_ok=True)\nsave_path = f\"saved_models/efficientnet_b0_{LEARNING_RATE}_model_3_32_aug_160ep.pth\"\ntorch.save(model.state_dict(), save_path)\nprint(f\"Continued training complete. Model saved to {save_path}\")\n","metadata":{"papermill":{"duration":16279.276239,"end_time":"2025-03-25T05:02:11.649854","exception":false,"start_time":"2025-03-25T00:30:52.373615","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Model performance Across Training Epochs","metadata":{}},{"cell_type":"code","source":"# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\n# Paths as saved in Kaggle\nmodel_paths = [\n    \"/kaggle/input/musicsheets/efficientnet_b0_0.005_model_3_32_aug_20ep.pth\",\n    \"/kaggle/input/musicsheets/efficientnet_b0_0.005_model_3_32_aug_50ep.pth\",\n    \"/kaggle/input/musicsheets/efficientnet_b0_0.005_model_3_32_aug_80ep.pth\",\n    \"/kaggle/input/musicsheets/efficientnet_b0_0.005_model_3_32_aug_100ep.pth\",\n    \"/kaggle/input/musicsheets/efficientnet_b0_0.0005_model_3_32_aug_115ep.pth\",\n    \"/kaggle/input/musicsheets/efficientnet_b0_0.0001_model_3_32_aug_150ep.pth\"\n]\n\n# Extract epoch number from path\ndef extract_epoch(path):\n    match = re.search(r\"_([0-9]+)ep\\.pth\", path)\n    return int(match.group(1)) if match else -1\n\n# Store results\nepoch_metrics = []\n\nfor path in sorted(model_paths, key=extract_epoch):\n    epoch_num = extract_epoch(path)\n\n    model = models.efficientnet_b0(pretrained=False)\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\n    model.load_state_dict(torch.load(path, map_location=device))\n    model = model.to(device)\n    model.eval()\n\n    val_losses, all_preds, all_labels = [], [], []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_losses.append(loss.item())\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    epoch_metrics.append({\n        \"epoch\": epoch_num,\n        \"val_loss\": avg_val_loss,\n        \"val_f1\": f1,\n        \"val_accuracy\": accuracy\n    })\n\ndf = pd.DataFrame(epoch_metrics).sort_values(\"epoch\")\n\n# Plot\nplt.figure(figsize=(18, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(df[\"epoch\"], df[\"val_loss\"], marker='o', color='salmon')\nplt.title(\"Validation Loss vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(1, 3, 2)\nplt.plot(df[\"epoch\"], df[\"val_f1\"], marker='o', color='skyblue')\nplt.title(\"Validation F1 Score vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1 Score\")\n\nplt.subplot(1, 3, 3)\nplt.plot(df[\"epoch\"], df[\"val_accuracy\"], marker='o', color='lightgreen')\nplt.title(\"Validation Accuracy vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"epoch_trend_aug_model.png\")\nplt.show()\n","metadata":{"papermill":{"duration":0.748754,"end_time":"2025-03-25T05:02:13.144990","exception":false,"start_time":"2025-03-25T05:02:12.396236","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Load model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_CLASSES = len(genre_to_idx)\ncriterion = nn.CrossEntropyLoss()\n\nmodel_path = \"/kaggle/input/musicsheets/efficientnet_b0_0.0001_model_3_32_aug_150ep.pth\"\nmodel = models.efficientnet_b0(pretrained=False)\nmodel.classifier[-1] = nn.Linear(model.classifier[-1].in_features, NUM_CLASSES)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel = model.to(device)\nmodel.eval()\n\n# Evaluate on test set\ntest_losses = []\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        test_losses.append(loss.item())\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Metrics\navg_loss = sum(test_losses) / len(test_losses)\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\naccuracy = accuracy_score(all_labels, all_preds)\n\n# Print\nprint(f\"Test Loss: {avg_loss:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\n\n# -----------------------------\n# Table of Metrics as Image\n# -----------------------------\nmetrics_df = pd.DataFrame({\n    \"Metric\": [\"Loss\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"],\n    \"Value\": [avg_loss, accuracy, f1, precision, recall]\n})\n\n# Save CSV (optional for download)\nmetrics_df.to_csv(\"test_metrics_summary.csv\", index=False)\n\n# Plot as table image\nfig, ax = plt.subplots(figsize=(6, 2))\nax.axis('off')\ntbl = ax.table(cellText=metrics_df.values, colLabels=metrics_df.columns, cellLoc='center', loc='center')\ntbl.scale(1, 2)\nplt.title(\"Test Set Performance Summary\", fontsize=14, weight='bold')\nplt.tight_layout()\nplt.savefig(\"test_metrics_summary.png\")\nplt.show()\n\n# -----------------------------\n# Confusion Matrix\n# -----------------------------\ncm = confusion_matrix(all_labels, all_preds)\nlabels = [idx_to_genre[i] for i in range(NUM_CLASSES)]\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(pd.DataFrame(cm, index=labels, columns=labels), \n            annot=True, fmt='d', cmap='Blues')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix on Test Set')\nplt.tight_layout()\nplt.savefig(\"confusion_matrix_test.png\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
