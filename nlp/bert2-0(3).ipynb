{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11140600,"sourceType":"datasetVersion","datasetId":6949295}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nimport json\nimport time\nimport math\nfrom collections import Counter\n\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom torch.utils.data import random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.560563Z","iopub.execute_input":"2025-03-27T11:41:21.560943Z","iopub.status.idle":"2025-03-27T11:41:21.567006Z","shell.execute_reply.started":"2025-03-27T11:41:21.560918Z","shell.execute_reply":"2025-03-27T11:41:21.565350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MusicXMLDataset(Dataset):\n    def __init__(self, json_path, tokenizer, max_len=512):\n        # Load the preprocessed entries from the JSON file\n        with open(json_path, 'r', encoding='utf-8') as f:\n            self.entries = json.load(f)\n        # (Optional) Filter entries if needed (using the same criteria as original)\n        self.entries = [entry for entry in self.entries if (\n            \"/mxl/0/\" in entry['mxl'] or\n            \"/mxl/1/\" in entry['mxl'] or\n            \"/mxl/2/\" in entry['mxl'] or\n            \"/mxl/3/\" in entry['mxl'] or\n            \"/mxl/4/\" in entry['mxl']\n        )]\n        # Enumerate unique genres and create a mapping to indices\n        unique_genres = sorted({entry['primary_genre'] for entry in self.entries})\n        print(\"PRIMARY GENRES:\", set(unique_genres))\n        self.genre_to_idx = {genre: idx for idx, genre in enumerate(unique_genres)}\n        self.idx_to_genre = {idx: genre for genre, idx in self.genre_to_idx.items()}\n        \n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.entries)\n\n    def __getitem__(self, idx):\n        entry = self.entries[idx]\n        # Parse the token list from the JSON field (it's stored as a JSON string)\n        tokens = json.loads(entry['tokens'])  # this yields a list of token strings\n        # Convert the list of tokens to the format expected by the tokenizer.\n        # We use the tokenizer to get input IDs and attention mask, padding to max_len.\n        encoding = self.tokenizer(\n            tokens, \n            is_split_into_words=True,       # treat the list of tokens as pre-split words\n            add_special_tokens=True,        # add [CLS], [SEP] as needed for the model\n            truncation=True, \n            padding='max_length', \n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        input_ids = encoding['input_ids'].squeeze(0)         # tensor of shape (max_len)\n        attention_mask = encoding['attention_mask'].squeeze(0)  # tensor of shape (max_len)\n        # Genre label to index\n        genre_str = entry['primary_genre']\n        label = torch.tensor(self.genre_to_idx[genre_str], dtype=torch.long)\n        return input_ids, attention_mask, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.568230Z","iopub.execute_input":"2025-03-27T11:41:21.568623Z","iopub.status.idle":"2025-03-27T11:41:21.748540Z","shell.execute_reply.started":"2025-03-27T11:41:21.568591Z","shell.execute_reply":"2025-03-27T11:41:21.747809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    for input_ids, attention_mask, labels in dataloader:\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.750013Z","iopub.execute_input":"2025-03-27T11:41:21.750255Z","iopub.status.idle":"2025-03-27T11:41:21.767970Z","shell.execute_reply.started":"2025-03-27T11:41:21.750229Z","shell.execute_reply":"2025-03-27T11:41:21.767070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in dataloader:\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = criterion(logits, labels)\n            total_loss += loss.item()\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n    accuracy = sum([p == l for p, l in zip(all_preds, all_labels)]) / len(all_labels)\n    return total_loss / len(dataloader), accuracy, precision, recall, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.769152Z","iopub.execute_input":"2025-03-27T11:41:21.769400Z","iopub.status.idle":"2025-03-27T11:41:21.780850Z","shell.execute_reply.started":"2025-03-27T11:41:21.769380Z","shell.execute_reply":"2025-03-27T11:41:21.780127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_curves(train_losses, val_losses, val_accuracies, val_precisions, val_recalls, val_f1s):\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(14, 5))\n\n    # Subplot 1: Losses\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Train Loss', linestyle='--', marker='o')\n    plt.plot(epochs, val_losses, label='Val Loss', linestyle='-', marker='o')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training & Validation Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Subplot 2: Metrics\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, val_accuracies, label='Accuracy', marker='o')\n    plt.plot(epochs, val_precisions, label='Precision', marker='o')\n    plt.plot(epochs, val_recalls, label='Recall', marker='o')\n    plt.plot(epochs, val_f1s, label='F1 Score', marker='o')\n    plt.xlabel('Epochs')\n    plt.ylabel('Score')\n    plt.title('Validation Metrics')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig(\"genre_curves.png\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.781623Z","iopub.execute_input":"2025-03-27T11:41:21.781887Z","iopub.status.idle":"2025-03-27T11:41:21.798072Z","shell.execute_reply.started":"2025-03-27T11:41:21.781859Z","shell.execute_reply":"2025-03-27T11:41:21.797307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_with_early_stopping(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, patience=3):\n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    val_precisions = []\n    val_recalls = []\n    val_f1s = []\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc, val_precision, val_recall, val_f1 = evaluate_model(model, val_loader, criterion, device)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n        val_precisions.append(val_precision)\n        val_recalls.append(val_recall)\n        val_f1s.append(val_f1)\n\n        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | \"\n              f\"P: {val_precision:.4f} | R: {val_recall:.4f} | F1: {val_f1:.4f}\")\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}\")\n                break\n\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n\n    plot_training_curves(train_losses, val_losses, val_accuracies, val_precisions, val_recalls, val_f1s)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.798935Z","iopub.execute_input":"2025-03-27T11:41:21.799209Z","iopub.status.idle":"2025-03-27T11:41:21.812008Z","shell.execute_reply.started":"2025-03-27T11:41:21.799181Z","shell.execute_reply":"2025-03-27T11:41:21.811270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set your seed\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.812715Z","iopub.execute_input":"2025-03-27T11:41:21.812978Z","iopub.status.idle":"2025-03-27T11:41:21.833303Z","shell.execute_reply.started":"2025-03-27T11:41:21.812959Z","shell.execute_reply":"2025-03-27T11:41:21.832735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose pre-trained model\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load your dataset\njson_path = \"/kaggle/input/mlx-dataset/preprocessed_dataset.json\"  # <-- put your JSON path here\ndataset = MusicXMLDataset(json_path, tokenizer, max_len=512)\nnum_classes = len(dataset.genre_to_idx)\n\n# Load pre-trained model with correct number of output classes\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n\n# Freeze all layers initially\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last layer (classifier)\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n    \n# Train-validation-test split\ntrain_size = int(0.8 * len(dataset))  # 80% for training\nval_size = int(0.1 * len(dataset))    # 10% for validation\ntest_size = len(dataset) - train_size - val_size  # Remaining 10% for testing\n\n# Split the dataset into train, validation, and test sets\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# DataLoaders for training, validation, and testing\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer and Loss\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:21.835033Z","iopub.execute_input":"2025-03-27T11:41:21.835222Z","iopub.status.idle":"2025-03-27T11:41:28.473405Z","shell.execute_reply.started":"2025-03-27T11:41:21.835205Z","shell.execute_reply":"2025-03-27T11:41:28.472520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = train_with_early_stopping(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=device,\n    num_epochs=100,\n    patience=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:41:28.474509Z","iopub.execute_input":"2025-03-27T11:41:28.474810Z","iopub.status.idle":"2025-03-27T12:08:20.125977Z","shell.execute_reply.started":"2025-03-27T11:41:28.474782Z","shell.execute_reply":"2025-03-27T12:08:20.124758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(trained_model.state_dict(), \"genre_classifier.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:08:20.126489Z","iopub.status.idle":"2025-03-27T12:08:20.126732Z","shell.execute_reply":"2025-03-27T12:08:20.126634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\n# Function to evaluate the model\ndef evaluate_model(model, test_dataloader, device):\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in test_dataloader:\n            inputs = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculating accuracy, precision, recall, f1 score\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    return accuracy, precision, recall, f1, cm\n\n# Function to plot the confusion matrix\ndef plot_confusion_matrix(cm, class_names):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.savefig(\"confusion_matrix\")\n    plt.show()\n\n# Example usage (after training the model and defining a test_dataloader):\naccuracy, precision, recall, f1, cm = evaluate_model(model, test_dataloader, device)\nplot_confusion_matrix(cm, class_names=unique_genres)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}