{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11140600,"sourceType":"datasetVersion","datasetId":6949295}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nimport json\nimport time\nimport math\nfrom collections import Counter\n\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, \n    precision_recall_fscore_support, \n    confusion_matrix, \n    classification_report\n)\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom torch.utils.data import random_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MusicXMLDataset(Dataset):\n    def __init__(self, json_path, tokenizer, max_len=512):\n        # Load the preprocessed entries from the JSON file\n        with open(json_path, 'r', encoding='utf-8') as f:\n            self.entries = json.load(f)\n        # Filter entries if needed \n        self.entries = [entry for entry in self.entries if (\n            \"/mxl/0/\" in entry['mxl'] or\n            \"/mxl/1/\" in entry['mxl'] or\n            \"/mxl/2/\" in entry['mxl'] or\n            \"/mxl/3/\" in entry['mxl'] or\n            \"/mxl/4/\" in entry['mxl']\n        )]\n        # Enumerate unique genres and create a mapping to indices\n        unique_genres = sorted({entry['primary_genre'] for entry in self.entries})\n        print(\"PRIMARY GENRES:\", set(unique_genres))\n        self.genre_to_idx = {genre: idx for idx, genre in enumerate(unique_genres)}\n        self.idx_to_genre = {idx: genre for genre, idx in self.genre_to_idx.items()}\n        \n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.entries)\n\n    def __getitem__(self, idx):\n        entry = self.entries[idx]\n        # Parse the token list from the JSON field (it's stored as a JSON string)\n        tokens = json.loads(entry['tokens'])  # this yields a list of token strings\n        # Convert the list of tokens to the format expected by the tokenizer.\n        # We use the tokenizer to get input IDs and attention mask, padding to max_len.\n        encoding = self.tokenizer(\n            tokens, \n            is_split_into_words=True,       # treat the list of tokens as pre-split words\n            add_special_tokens=True,        # add [CLS], [SEP] as needed for the model\n            truncation=True, \n            padding='max_length', \n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        input_ids = encoding['input_ids'].squeeze(0)         # tensor of shape (max_len)\n        attention_mask = encoding['attention_mask'].squeeze(0)  # tensor of shape (max_len)\n        # Genre label to index\n        genre_str = entry['primary_genre']\n        label = torch.tensor(self.genre_to_idx[genre_str], dtype=torch.long)\n        return input_ids, attention_mask, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    for input_ids, attention_mask, labels in dataloader:\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in dataloader:\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = criterion(logits, labels)\n            total_loss += loss.item()\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n    accuracy = sum([p == l for p, l in zip(all_preds, all_labels)]) / len(all_labels)\n    return total_loss / len(dataloader), accuracy, precision, recall, f1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_curves(train_losses, val_losses, val_accuracies, val_precisions, val_recalls, val_f1s):\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(14, 5))\n\n    # Subplot 1: Losses\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Train Loss', linestyle='--', marker='o')\n    plt.plot(epochs, val_losses, label='Val Loss', linestyle='-', marker='o')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training & Validation Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Subplot 2: Metrics\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, val_accuracies, label='Accuracy', marker='o')\n    plt.plot(epochs, val_precisions, label='Precision', marker='o')\n    plt.plot(epochs, val_recalls, label='Recall', marker='o')\n    plt.plot(epochs, val_f1s, label='F1 Score', marker='o')\n    plt.xlabel('Epochs')\n    plt.ylabel('Score')\n    plt.title('Validation Metrics')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig(\"genre_curves.png\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_with_early_stopping(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, patience=3):\n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    val_precisions = []\n    val_recalls = []\n    val_f1s = []\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc, val_precision, val_recall, val_f1 = evaluate_model(model, val_loader, criterion, device)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n        val_precisions.append(val_precision)\n        val_recalls.append(val_recall)\n        val_f1s.append(val_f1)\n\n        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | \"\n              f\"P: {val_precision:.4f} | R: {val_recall:.4f} | F1: {val_f1:.4f}\")\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}\")\n                break\n\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n\n    plot_training_curves(train_losses, val_losses, val_accuracies, val_precisions, val_recalls, val_f1s)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set your seed\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose pre-trained model\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load your dataset\njson_path = \"/kaggle/input/mlx-dataset/preprocessed_dataset.json\"  # <-- put your JSON path here\ndataset = MusicXMLDataset(json_path, tokenizer, max_len=512)\nnum_classes = len(dataset.genre_to_idx)\n\n# Load pre-trained model with correct number of output classes\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n\n# Freeze all layers\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last 2 layers of DistilBERT encoder\nfor layer in model.base_model.transformer.layer[-1:]:\n    for param in layer.parameters():\n        param.requires_grad = True\n    \n# Train-validation-test split\ntrain_size = int(0.8 * len(dataset))  # 80% for training\nval_size = int(0.1 * len(dataset))    # 10% for validation\ntest_size = len(dataset) - train_size - val_size  # Remaining 10% for testing\n\n# Split the dataset into train, validation, and test sets\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# DataLoaders for training, validation, and testing\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer and Loss\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = train_with_early_stopping(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=device,\n    num_epochs=100,\n    patience=5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(trained_model.state_dict(), \"genre_classifier.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_dataloader, device):\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in test_dataloader:\n            # Unpack the batch for this specific dataset structure\n            inputs, attention_mask, labels = batch\n            inputs = inputs.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            # Perform model inference\n            outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Get unique labels to pass to precision_recall_fscore_support\n    unique_labels = np.unique(np.concatenate((all_labels, all_preds)))\n    \n    # Calculating metrics with zero_division to handle labels with no predictions\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, support = precision_recall_fscore_support(\n        all_labels, \n        all_preds, \n        labels=unique_labels,\n        average=None,  # Get per-class metrics\n        zero_division=0  # Set to 0 instead of raising a warning\n    )\n\n    # Weighted average metrics\n    w_precision, w_recall, w_f1, _ = precision_recall_fscore_support(\n        all_labels, \n        all_preds, \n        average='weighted', \n        zero_division=0\n    )\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds, labels=unique_labels)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'support': support,\n        'weighted_precision': w_precision,\n        'weighted_recall': w_recall,\n        'weighted_f1': w_f1,\n        'confusion_matrix': cm,\n        'predictions': all_preds,\n        'true_labels': all_labels\n    }\n\ndef plot_confusion_matrix(cm, class_names=None, title='Confusion Matrix', normalize=False):\n    plt.figure(figsize=(16, 12))\n    \n    # If no class names provided, use numeric labels\n    if class_names is None:\n        class_names = [str(i) for i in range(cm.shape[0])]\n    \n    if normalize:\n        # Normalize the confusion matrix\n        cm_plot = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        fmt = '.2%'\n        cbar_label = 'Normalized Percentage'\n        plot_title = f'{title} - Normalized Percentages'\n    else:\n        cm_plot = cm\n        fmt = 'd'\n        cbar_label = 'Absolute Count'\n        plot_title = f'{title} - Absolute Counts'\n    \n    sns.heatmap(cm_plot, \n                annot=True,  \n                fmt=fmt,     \n                cmap='YlGnBu', \n                xticklabels=class_names, \n                yticklabels=class_names,\n                cbar_kws={'label': cbar_label})\n    plt.title(plot_title)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    # Save with a descriptive filename\n    filename = f\"confusion_matrix_{'normalized' if normalize else 'absolute'}.png\"\n    plt.savefig(filename, bbox_inches='tight', dpi=300)\n    plt.show()\n\ndef print_detailed_metrics(evaluation_results, class_names=None):\n    \"\"\"\n    Prints a comprehensive breakdown of model performance metrics.\n    \n    Args:\n        evaluation_results (dict): Dictionary containing evaluation metrics\n        class_names (list, optional): List of class names corresponding to indices\n    \"\"\"\n    print(\"\\n===== Model Performance Metrics =====\")\n    print(f\"Overall Accuracy: {evaluation_results['accuracy']:.4f}\")\n    print(f\"Weighted Precision: {evaluation_results['weighted_precision']:.4f}\")\n    print(f\"Weighted Recall: {evaluation_results['weighted_recall']:.4f}\")\n    print(f\"Weighted F1-Score: {evaluation_results['weighted_f1']:.4f}\")\n    \n    print(\"\\n===== Per-Class Metrics =====\")\n    if class_names is None:\n        class_names = [str(i) for i in range(len(evaluation_results['precision']))]\n    \n    # Create a formatted table of per-class metrics\n    print(f\"{'Class':<20} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n    print(\"-\" * 70)\n    for i, (name, p, r, f1, sup) in enumerate(zip(\n        class_names, \n        evaluation_results['precision'], \n        evaluation_results['recall'], \n        evaluation_results['f1'], \n        evaluation_results['support']\n    )):\n        print(f\"{name:<20} {p:10.4f} {r:10.4f} {f1:10.4f} {sup:10.0f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get class names from the dataset\nclass_names = list(test_dataset.dataset.genre_to_idx.keys())\n\nevaluation_results = evaluate_model(trained_model, test_loader, device)\n\nplot_confusion_matrix(\n    evaluation_results['confusion_matrix'], \n    class_names=class_names, \n    title='Music Genre Classification',\n    normalize=False \n)\n\nplot_confusion_matrix(\n    evaluation_results['confusion_matrix'], \n    class_names=class_names, \n    title='Music Genre Classification Normalized',\n    normalize=True  # Normalized percentages\n)\n\nprint_detailed_metrics(evaluation_results, class_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}